import re
from extractors import crawl_rss_direct, crawl_list_page
from tools import (
    send_wecom_markdown,
    save_markdown,
    summarize_article,
    fetch_images_for_article,
    insert_images_smart,
    insert_images_to_content,
    push_to_feishu,
    save_processed_url,
)
from config import ENABLE_IMAGE_INSERTION, IMAGE_COUNT, USE_AI_IMAGE_GENERATION, USE_SMART_INSERTION


def enrich_images(result):
    """è‡ªåŠ¨è·å–å¹¶æ’å…¥å›¾ç‰‡åˆ°æ­£æ–‡"""
    if not ENABLE_IMAGE_INSERTION:
        return
    print(f"\nğŸ–¼ï¸ å¼€å§‹è·å–å›¾ç‰‡ï¼ˆæ•°é‡: {IMAGE_COUNT}ï¼‰...")
    try:
        image_urls = fetch_images_for_article(
            result["title"],
            result["content"],
            count=IMAGE_COUNT,
            use_ai_generation=USE_AI_IMAGE_GENERATION
        )
        if image_urls:
            print(f"  âœ… æˆåŠŸè·å– {len(image_urls)} å¼ å›¾ç‰‡")
            if USE_SMART_INSERTION:
                result["content"] = insert_images_smart(
                    result["content"], image_urls, result["title"]
                )
            else:
                result["content"] = insert_images_to_content(
                    result["content"], image_urls
                )
            print(f"  âœ… å›¾ç‰‡å·²æ’å…¥åˆ°æ­£æ–‡")
        else:
            print(f"  âš ï¸ æœªèƒ½è·å–åˆ°å›¾ç‰‡ï¼Œç»§ç»­ä¿å­˜æ–‡ç« ")
    except Exception as e:
        print(f"  âš ï¸ å›¾ç‰‡è·å–/æ’å…¥è¿‡ç¨‹å‡ºé”™: {e}ï¼Œç»§ç»­ä¿å­˜æ–‡ç« ")


def extract_title_and_cover(result):
    """ä»æ­£æ–‡ä¸­æå–æœ€ç»ˆæ ‡é¢˜ã€å°é¢å›¾å’Œæ‘˜è¦"""
    title_match = re.search(r'^##\s+(.+)$', result["content"], re.MULTILINE)
    if title_match:
        result["title"] = title_match.group(1).strip()

    if not result.get("image_url"):
        img_match = re.search(r'!\[.*?\]\((.*?)\)', result["content"])
        if img_match:
            result["image_url"] = img_match.group(1)

    # ä» LLM ç”Ÿæˆçš„å†…å®¹ä¸­æå–ç¬¬ä¸€æ®µä½œä¸ºæ‘˜è¦
    content = result.get("content", "")
    paragraphs = [p.strip() for p in content.split("\n\n") if p.strip() and not p.strip().startswith(("#", "!", ">"))]
    if paragraphs:
        result["summary"] = paragraphs[0]


def crawl_article(source="qbitai"):
    """æŠ“å–æ–‡ç« å¹¶ç”¨å¤§æ¨¡å‹æ€»ç»“"""
    if source == "techcrunch":
        feed_url = "https://techcrunch.com/feed/"
        tags = ["ai", "machine learning", "deep learning","LLM","AI Agent","AI","å¤§æ¨¡å‹","Agent","è§†é¢‘","OpenAI"]
        result = crawl_rss_direct(tags, feed_url)
    elif source == "qbitai":
        list_url = "https://www.qbitai.com/category/%e8%b5%84%e8%ae%af"
        tags = ["AI", "å¤§æ¨¡å‹", "ç®—åŠ›", "è§†é¢‘", "OpenAI","AI Agent","Agent","LLM"]
        result = crawl_list_page(tags, list_url)
    else:
        raise ValueError(f"æœªçŸ¥çš„æŠ“å–æº: {source}")

    if not result:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡ç« ")
        return None

    result["content"] = summarize_article(result["title"], result["content"])
    return result


def process_and_publish(source="qbitai", send_wecom=False):
    """æŠ“å–æ–‡ç«  â†’ æ’å…¥å›¾ç‰‡ â†’ æå–æ ‡é¢˜å°é¢ â†’ ä¿å­˜æœ¬åœ° â†’ æ¨é€é£ä¹¦ â†’ å‘é€å¾®ä¿¡ç¾¤ï¼ˆå¯é€‰ï¼‰"""
    result = crawl_article(source=source)
    if not result:
        return None

    enrich_images(result)
    extract_title_and_cover(result)
    save_markdown(result)
    push_to_feishu(result)
    save_processed_url(result["url"])

    if send_wecom:
        send_wecom_markdown(result["content"])

    return result



