from typing import Optional, List, Dict
import asyncio
import re

try:
    from crawl4ai import AsyncWebCrawler  # type: ignore
except ImportError:  # pragma: no cover
    AsyncWebCrawler = None  # type: ignore


async def _fetch_with_crawl4ai(url: str):
    """ä½¿ç”¨ crawl4ai å¼‚æ­¥æŠ“å–é¡µé¢ï¼Œè¿”å›å®Œæ•´ç»“æœå¯¹è±¡ã€‚"""
    if AsyncWebCrawler is None:
        raise RuntimeError("crawl4ai æœªå®‰è£…ï¼Œè¯·å…ˆæ‰§è¡Œ `pip install crawl4ai`")
    async with AsyncWebCrawler() as crawler:  # type: ignore[operator]
        result = await crawler.arun(url=url)
        return result


def fetch_html(url: str) -> str:
    """
    ä½¿ç”¨ crawl4ai è·å–é¡µé¢çš„åŸå§‹ HTMLã€‚
    ç”¨äºéœ€è¦ç›´æ¥æ“ä½œ HTML çš„åœºæ™¯ï¼ˆå¦‚æå–å›¾ç‰‡ç­‰ï¼‰ã€‚
    """
    try:
        result = asyncio.run(_fetch_with_crawl4ai(url))
        return getattr(result, "html", "") or ""
    except Exception as e:
        print(f"  âŒ è·å– HTML å¤±è´¥: {e}")
        return ""


def extract_article_content(url: str) -> Optional[str]:
    """
    ä½¿ç”¨ crawl4ai æå–æ–‡ç« æ­£æ–‡å†…å®¹ï¼ˆæ›¿ä»£ trafilaturaï¼‰ã€‚
    è¿”å›æ¸…ç†åçš„æ–‡æœ¬å†…å®¹ã€‚
    """
    print(f"  ğŸ” æ­£åœ¨ä½¿ç”¨ crawl4ai æå–æ–‡ç« å†…å®¹: {url}")
    try:
        result = asyncio.run(_fetch_with_crawl4ai(url))
        # crawl4ai é€šå¸¸æä¾› markdown æˆ– cleaned_htmlï¼Œä¼˜å…ˆç”¨ markdown
        content = getattr(result, "markdown", "") or getattr(result, "cleaned_html", "")
        if content:
            # å¦‚æœè¿”å›çš„æ˜¯ HTMLï¼Œç®€å•æå–æ–‡æœ¬
            if content.startswith("<"):
                # ç®€å•å»é™¤ HTML æ ‡ç­¾
                content = re.sub(r"<[^>]+>", "", content)
                content = re.sub(r"\s+", " ", content).strip()
            print(f"  âœ… æå–æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            return content
        else:
            print("  âš ï¸ æå–å¤±è´¥ï¼Œæœªè·å–åˆ°å†…å®¹")
            return None
    except Exception as e:
        print(f"  âŒ æå–å‡ºé”™: {e}")
        return None


def extract_links_from_page(url: str, max_links: int = 10) -> List[Dict[str, str]]:
    """
    ä½¿ç”¨ crawl4ai ä»åˆ—è¡¨é¡µæå–æ–‡ç« é“¾æ¥å’Œæ ‡é¢˜ï¼ˆæ›¿ä»£ BeautifulSoupï¼‰ã€‚
    è¿”å› [{"url": "...", "title": "..."}, ...]
    """
    print(f"  ğŸ“‹ æ­£åœ¨ä½¿ç”¨ crawl4ai æå–é“¾æ¥: {url}")
    try:
        result = asyncio.run(_fetch_with_crawl4ai(url))
        html = getattr(result, "html", "") or ""
        
        if not html:
            return []
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ä» HTML ä¸­æå–é“¾æ¥å’Œæ ‡é¢˜
        # åŒ¹é…å¸¸è§çš„æ–‡ç« é“¾æ¥æ¨¡å¼ï¼š<a href="..." ...>æ ‡é¢˜</a>
        candidates: List[Dict[str, str]] = []
        seen_urls = set()
        
        # åŒ¹é… <a> æ ‡ç­¾å†…çš„é“¾æ¥å’Œæ–‡æœ¬
        pattern = r'<a[^>]+href=["\']([^"\']+)["\'][^>]*>([^<]+)</a>'
        matches = re.finditer(pattern, html, re.IGNORECASE)
        
        for match in matches:
            href = match.group(1)
            title = match.group(2).strip()
            
            # è¿‡æ»¤æ‰æ— æ•ˆé“¾æ¥
            if not href or not title or len(title) < 5:
                continue
            
            # å¤„ç†ç›¸å¯¹é“¾æ¥
            if href.startswith("/"):
                href = f"{url.split('/')[0]}//{url.split('/')[2]}{href}"
            elif not href.startswith("http"):
                continue
            
            # å»é‡
            if href in seen_urls:
                continue
            seen_urls.add(href)
            
            candidates.append({"url": href, "title": title})
            if len(candidates) >= max_links:
                break
        
        print(f"  âœ… æå–åˆ° {len(candidates)} ä¸ªé“¾æ¥")
        return candidates
    except Exception as e:
        print(f"  âŒ æå–é“¾æ¥å‡ºé”™: {e}")
        return []

