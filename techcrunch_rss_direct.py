from typing import List, Optional, Dict, Any
from datetime import datetime
import re

import feedparser
import requests
import trafilatura

from config import MODEL, TIME_WINDOW_HOURS, MIN_CONTENT_LENGTH, client


def fetch_feed(feed_url: str):
    print(f"ğŸ“¡ æ­£åœ¨è·å– RSS feed: {feed_url}")
    try:
        response = requests.get(feed_url, timeout=15, verify=True)
        response.raise_for_status()
        print(f"  âœ… HTTP è¯·æ±‚æˆåŠŸï¼ŒçŠ¶æ€ç : {response.status_code}")
        feed = feedparser.parse(response.content)
        print(f"âœ… è§£ææˆåŠŸï¼Œå…± {len(feed.entries)} ç¯‡æ–‡ç« ")
        if feed.bozo:
            print(f"âš ï¸ RSS è§£æè­¦å‘Š: {feed.bozo_exception}")
        if feed.entries:
            print(f"ğŸ“° Feed æ ‡é¢˜: {feed.feed.get('title', 'Unknown')}")
        return feed
    except requests.exceptions.SSLError:
        print("âš ï¸ SSL è¯ä¹¦éªŒè¯å¤±è´¥ï¼Œå°è¯•ç¦ç”¨éªŒè¯...")
        response = requests.get(feed_url, timeout=15, verify=False)
        response.raise_for_status()
        feed = feedparser.parse(response.content)
        print(f"âœ… è§£ææˆåŠŸï¼ˆå·²ç¦ç”¨ SSL éªŒè¯ï¼‰ï¼Œå…± {len(feed.entries)} ç¯‡æ–‡ç« ")
        return feed
    except Exception as e:
        print(f"âŒ è·å– feed å¤±è´¥: {e}")
        raise


def is_recent(entry):
    if not entry.get("published_parsed"):
        print(f"  âš ï¸ æ–‡ç« æ— å‘å¸ƒæ—¶é—´ä¿¡æ¯: {entry.get('title', 'Unknown')}")
        return False
    published = datetime(*entry.published_parsed[:6])
    delta = datetime.utcnow() - published
    hours_ago = delta.total_seconds() / 3600
    is_recent_flag = delta.total_seconds() < TIME_WINDOW_HOURS * 3600
    print(
        f"  ğŸ“… å‘å¸ƒæ—¶é—´: {published.strftime('%Y-%m-%d %H:%M:%S')}, "
        f"{hours_ago:.1f} å°æ—¶å‰, "
        f"{'âœ… åœ¨æ—¶é—´çª—å£å†…' if is_recent_flag else 'âŒ è¶…å‡ºæ—¶é—´çª—å£'}"
    )
    return is_recent_flag


def extract_article(url: str) -> Optional[str]:
    print(f"  ğŸ” æ­£åœ¨æå–æ–‡ç« å†…å®¹: {url}")
    try:
        html = requests.get(url, timeout=15).text
        content = trafilatura.extract(html)
        if content:
            print(f"  âœ… æå–æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
        else:
            print("  âš ï¸ æå–å¤±è´¥ï¼Œæœªè·å–åˆ°å†…å®¹")
        return content
    except Exception as e:
        print(f"  âŒ æå–å‡ºé”™: {e}")
        return None


def _extract_image_from_entry(entry):
    """ä» RSS entry ä¸­å°½é‡æå–å›¾ç‰‡ URLã€‚"""
    image_url = None

    if hasattr(entry, "media_content") and entry.media_content:
        image_url = entry.media_content[0].get("url")
    elif hasattr(entry, "media_thumbnail") and entry.media_thumbnail:
        image_url = entry.media_thumbnail[0].get("url")
    else:
        html = getattr(entry, "summary", "") or ""
        m = re.search(r'<img[^>]+src=["\']([^"\']+)["\']', html, re.IGNORECASE)
        if m:
            image_url = m.group(1)

    return image_url


def _fetch_html(url: str) -> str:
    print(f"  ğŸŒ æ­£åœ¨è·å– HTML: {url}")
    try:
        resp = requests.get(url, timeout=15)
        resp.raise_for_status()
        return resp.text
    except Exception as e:
        print(f"  âŒ è·å– HTML å¤±è´¥: {e}")
        return ""


def _extract_image_from_html(html: str) -> Optional[str]:
    if not html:
        return None

    og_match = re.search(
        r'<meta[^>]+property=["\']og:image["\'][^>]+content=["\']([^"\']+)["\']',
        html,
        re.IGNORECASE,
    )
    if og_match:
        return og_match.group(1)

    img_match = re.search(
        r'<img[^>]+src=["\']([^"\']+)["\']',
        html,
        re.IGNORECASE,
    )
    if img_match:
        return img_match.group(1)

    return None


def get_image_url(entry, url: str) -> Optional[str]:
    """ç»¼åˆ RSS ä¸ç›´æ¥çˆ¬å–ä¸¤ç§æ–¹å¼è·å–å›¾ç‰‡ URLã€‚"""
    image_url = None

    try:
        image_url = _extract_image_from_entry(entry)
    except Exception as e:
        print(f"  âš ï¸ ä» RSS æå–å›¾ç‰‡å¤±è´¥: {e}")

    if not image_url:
        html = _fetch_html(url)
        image_url = _extract_image_from_html(html)

    if image_url:
        print(f"  ğŸ–¼ï¸ å›¾ç‰‡ URL: {image_url}")
    else:
        print("  âš ï¸ æœªæ‰¾åˆ°å›¾ç‰‡ URL")

    return image_url


def summarize(title: str, content: str) -> str:
    print(f"  ğŸ¤– æ­£åœ¨ä½¿ç”¨ {MODEL} ç”Ÿæˆæ‘˜è¦...")
    prompt = f"""
ä½ æ˜¯ä¸€åç§‘æŠ€åª’ä½“ç¼–è¾‘ã€‚

è¯·å°†ä¸‹é¢çš„ TechCrunch æ–°é—»æ•´ç†æˆä¸€ç¯‡ä¸­æ–‡ç§‘æŠ€æ–‡ç« ï¼š
- ä¸è¦é€å¥ç¿»è¯‘
- ä¿ç•™æ ¸å¿ƒäº‹å®
- é€‚å½“è¡¥å……èƒŒæ™¯
- è¯´æ˜è¿™æ¡æ–°é—»ä¸ºä»€ä¹ˆé‡è¦
- 400~600 å­—
- é£æ ¼ï¼šç†æ€§ã€ä¸“ä¸šã€åæŠ€æœ¯

æ ‡é¢˜ï¼š{title}

æ­£æ–‡ï¼š
{content}
"""
    try:
        resp = client.chat.completions.create(
            model=MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.4,
        )
        summary = resp.choices[0].message.content.strip()
        print(f"  âœ… æ‘˜è¦ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(summary)} å­—ç¬¦")
        return summary
    except Exception as e:
        print(f"  âŒ æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
        raise


def _is_related(entry, tags: List[str]) -> bool:
    """æ ¹æ®ä¼ å…¥çš„å…³é”®è¯åˆ—è¡¨åˆ¤æ–­æ–‡ç« æ˜¯å¦ç›¸å…³ã€‚"""
    title = (getattr(entry, "title", "") or "").lower()
    summary = (getattr(entry, "summary", "") or "").lower()
    text = f"{title}\n{summary}"

    for kw in tags:
        if kw.lower() in text:
            print(f"  âœ… åˆ¤å®šä¸ºç›®æ ‡é¢†åŸŸç›¸å…³ï¼ˆå‘½ä¸­å…³é”®è¯: {kw}ï¼‰")
            return True

    print("  â›” éç›®æ ‡æ ‡ç­¾ç›¸å…³æ–‡ç« ï¼Œè·³è¿‡")
    return False


def crawl_techcrunch_rss_direct(
    article_tags: List[str],
    feed_url: str,
) -> Optional[Dict[str, Any]]:
    """
    ä½¿ç”¨ TechCrunch RSS + HTML + LLM æµç¨‹ï¼Œå®ç°æœ€ç®€å•çš„æŠ“å–ç­–ç•¥ï¼š
    - éå† RSS ä¸­çš„æ–‡ç« 
    - ç”¨è°ƒç”¨æ–¹ç»™å‡ºçš„ article_tags åšç²—è¿‡æ»¤
    - ç”¨ is_recent åšæ—¶é—´è¿‡æ»¤
    - æŠ“æ­£æ–‡ã€æŠ“å›¾ç‰‡ã€ç”Ÿæˆæ‘˜è¦
    - è¿”å›ç¬¬ä¸€ç¯‡ç¬¦åˆæ¡ä»¶çš„æ–‡ç« æ‘˜è¦ç»“æœ
    """
    feed = fetch_feed(feed_url)

    for entry in feed.entries:
        title = getattr(entry, "title", "Unknown")
        url = getattr(entry, "link", "")

        print(f"\næ£€æŸ¥æ–‡ç« : {title}")

        if not _is_related(entry, article_tags):
            continue

        if not is_recent(entry):
            print("  â­ï¸ è·³è¿‡ï¼ˆä¸åœ¨æ—¶é—´çª—å£å†…ï¼‰")
            continue

        image_url = get_image_url(entry, url)
        content = extract_article(url)
        if not content or len(content) < MIN_CONTENT_LENGTH:
            print(
                f"  âš ï¸ å†…å®¹è¿‡çŸ­ï¼ˆ{len(content) if content else 0} å­—ç¬¦ < {MIN_CONTENT_LENGTH}ï¼‰ï¼Œè·³è¿‡"
            )
            continue

        summary = summarize(title, content)

        return {
            "title": title,
            "content": summary,
            "image_url": image_url,
            "url": url,
        }

    print("â— æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡ç« ")
    return None


