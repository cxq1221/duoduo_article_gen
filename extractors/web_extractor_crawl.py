from typing import Optional, List, Dict
import asyncio
import re

try:
    from crawl4ai import AsyncWebCrawler  # type: ignore
except ImportError:  # pragma: no cover
    AsyncWebCrawler = None  # type: ignore


async def _fetch_with_crawl4ai(url: str):
    """ä½¿ç”¨ crawl4ai å¼‚æ­¥æŠ“å–é¡µé¢ï¼Œè¿”å›å®Œæ•´ç»“æœå¯¹è±¡ã€‚"""
    if AsyncWebCrawler is None:
        raise RuntimeError("crawl4ai æœªå®‰è£…ï¼Œè¯·å…ˆæ‰§è¡Œ `pip install crawl4ai`")
    async with AsyncWebCrawler() as crawler:  # type: ignore[operator]
        result = await crawler.arun(url=url)
        return result


def fetch_html(url: str) -> str:
    """
    ä½¿ç”¨ crawl4ai è·å–é¡µé¢çš„åŸå§‹ HTMLã€‚
    ç”¨äºéœ€è¦ç›´æ¥æ“ä½œ HTML çš„åœºæ™¯ï¼ˆå¦‚æå–å›¾ç‰‡ç­‰ï¼‰ã€‚
    """
    try:
        result = asyncio.run(_fetch_with_crawl4ai(url))
        return getattr(result, "html", "") or ""
    except Exception as e:
        print(f"  âŒ è·å– HTML å¤±è´¥: {e}")
        return ""


def extract_article_content(url: str) -> Optional[str]:
    """
    ä½¿ç”¨ crawl4ai æå–æ–‡ç« æ­£æ–‡å†…å®¹ï¼ˆæ›¿ä»£ trafilaturaï¼‰ã€‚
    è¿”å›æ¸…ç†åçš„æ–‡æœ¬å†…å®¹ã€‚
    """
    print(f"  ğŸ” æ­£åœ¨ä½¿ç”¨ crawl4ai æå–æ–‡ç« å†…å®¹: {url}")
    try:
        result = asyncio.run(_fetch_with_crawl4ai(url))
        # crawl4ai é€šå¸¸æä¾› markdown æˆ– cleaned_htmlï¼Œä¼˜å…ˆç”¨ markdown
        content = getattr(result, "markdown", "") or getattr(result, "cleaned_html", "")
        if content:
            # å¦‚æœè¿”å›çš„æ˜¯ HTMLï¼Œç®€å•æå–æ–‡æœ¬
            if content.startswith("<"):
                # ç®€å•å»é™¤ HTML æ ‡ç­¾
                content = re.sub(r"<[^>]+>", "", content)
                content = re.sub(r"\s+", " ", content).strip()
            print(f"  âœ… æå–æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            return content
        else:
            print("  âš ï¸ æå–å¤±è´¥ï¼Œæœªè·å–åˆ°å†…å®¹")
            return None
    except Exception as e:
        print(f"  âŒ æå–å‡ºé”™: {e}")
        return None


def extract_links_from_page(url: str, max_links: int = 10) -> List[Dict[str, str]]:
    """
    ä½¿ç”¨ crawl4ai å†…ç½®çš„é“¾æ¥æå–ä»åˆ—è¡¨é¡µè·å–æ–‡ç« é“¾æ¥å’Œæ ‡é¢˜ã€‚
    è¿”å› [{"url": "...", "title": "..."}, ...]
    """
    print(f"  ğŸ“‹ æ­£åœ¨ä½¿ç”¨ crawl4ai æå–é“¾æ¥: {url}")
    try:
        result = asyncio.run(_fetch_with_crawl4ai(url))
        links = getattr(result, "links", {})
        internal = links.get("internal", [])

        candidates: List[Dict[str, str]] = []
        seen_urls: set = set()

        for link in internal:
            href = link.get("href", "")
            title = link.get("text", "").strip()

            if not href or not title or len(title) < 5:
                continue

            # å»é‡
            if href in seen_urls:
                continue
            seen_urls.add(href)

            candidates.append({"url": href, "title": title})
            if len(candidates) >= max_links:
                break

        print(f"  âœ… æå–åˆ° {len(candidates)} ä¸ªé“¾æ¥")
        return candidates
    except Exception as e:
        print(f"  âŒ æå–é“¾æ¥å‡ºé”™: {e}")
        return []

