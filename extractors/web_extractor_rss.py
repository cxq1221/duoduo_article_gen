from typing import List, Optional, Dict, Any
from datetime import datetime
import re

import feedparser
import requests
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥ config
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import TIME_WINDOW_HOURS, MIN_CONTENT_LENGTH
from .web_extractor_crawl import extract_article_content, fetch_html
from tools import match_tags
import html


def _fetch_feed(feed_url: str):
    print(f"ğŸ“¡ æ­£åœ¨è·å– RSS feed: {feed_url}")
    try:
        response = requests.get(feed_url, timeout=15, verify=True)
        response.raise_for_status()
        print(f"  âœ… HTTP è¯·æ±‚æˆåŠŸï¼ŒçŠ¶æ€ç : {response.status_code}")
        feed = feedparser.parse(response.content)
        print(f"âœ… è§£ææˆåŠŸï¼Œå…± {len(feed.entries)} ç¯‡æ–‡ç« ")
        if feed.bozo:
            print(f"âš ï¸ RSS è§£æè­¦å‘Š: {feed.bozo_exception}")
        if feed.entries:
            print(f"ğŸ“° Feed æ ‡é¢˜: {feed.feed.get('title', 'Unknown')}")
        return feed
    except requests.exceptions.SSLError:
        print("âš ï¸ SSL è¯ä¹¦éªŒè¯å¤±è´¥ï¼Œå°è¯•ç¦ç”¨éªŒè¯...")
        response = requests.get(feed_url, timeout=15, verify=False)
        response.raise_for_status()
        feed = feedparser.parse(response.content)
        print(f"âœ… è§£ææˆåŠŸï¼ˆå·²ç¦ç”¨ SSL éªŒè¯ï¼‰ï¼Œå…± {len(feed.entries)} ç¯‡æ–‡ç« ")
        return feed
    except Exception as e:
        print(f"âŒ è·å– feed å¤±è´¥: {e}")
        raise


def is_recent(entry):
    if not entry.get("published_parsed"):
        print(f"  âš ï¸ æ–‡ç« æ— å‘å¸ƒæ—¶é—´ä¿¡æ¯: {entry.get('title', 'Unknown')}")
        return False
    published = datetime(*entry.published_parsed[:6])
    delta = datetime.utcnow() - published
    hours_ago = delta.total_seconds() / 3600
    is_recent_flag = delta.total_seconds() < TIME_WINDOW_HOURS * 3600
    print(
        f"  ğŸ“… å‘å¸ƒæ—¶é—´: {published.strftime('%Y-%m-%d %H:%M:%S')}, "
        f"{hours_ago:.1f} å°æ—¶å‰, "
        f"{'âœ… åœ¨æ—¶é—´çª—å£å†…' if is_recent_flag else 'âŒ è¶…å‡ºæ—¶é—´çª—å£'}"
    )
    return is_recent_flag


def _extract_image_from_entry(entry):
    """ä» RSS entry ä¸­å°½é‡æå–å›¾ç‰‡ URLã€‚"""
    print(f"  ğŸ–¼ï¸ æ­£åœ¨ä» RSS entry ä¸­æå–å›¾ç‰‡ URL")
    image_url = None

    if hasattr(entry, "media_content") and entry.media_content:
        image_url = entry.media_content[0].get("url")
    elif hasattr(entry, "media_thumbnail") and entry.media_thumbnail:
        image_url = entry.media_thumbnail[0].get("url")
    else:
        html = getattr(entry, "summary", "") or ""
        m = re.search(r'<img[^>]+src=["\']([^"\']+)["\']', html, re.IGNORECASE)
        if m:
            image_url = m.group(1)
    print(f"  ğŸ–¼ï¸  image_url: {image_url}")
    return image_url


def _extract_image_from_html(html: str) -> Optional[str]:
    if not html:
        return None

    og_match = re.search(
        r'<meta[^>]+property=["\']og:image["\'][^>]+content=["\']([^"\']+)["\']',
        html,
        re.IGNORECASE,
    )
    if og_match:
        return og_match.group(1)

    img_match = re.search(
        r'<img[^>]+src=["\']([^"\']+)["\']',
        html,
        re.IGNORECASE,
    )
    if img_match:
        return img_match.group(1)

    return None


def get_image_url(entry, url: str) -> Optional[str]:
    """ç»¼åˆ RSS ä¸ç›´æ¥çˆ¬å–ä¸¤ç§æ–¹å¼è·å–å›¾ç‰‡ URLã€‚"""
    image_url = None

    try:
        image_url = _extract_image_from_entry(entry)
    except Exception as e:
        print(f"  âš ï¸ ä» RSS æå–å›¾ç‰‡å¤±è´¥: {e}")

    if not image_url:
        print(f"  ğŸŒ æ­£åœ¨è·å– HTML: {url}")
        html = fetch_html(url)
        image_url = _extract_image_from_html(html)

    if image_url:
        print(f"  ğŸ–¼ï¸ å›¾ç‰‡ URL: {image_url}")
    else:
        print("  âš ï¸ æœªæ‰¾åˆ°å›¾ç‰‡ URL")

    return image_url


def _extract_content_from_entry(entry) -> Optional[str]:
    """
    ä» RSS entry ä¸­æå–æ–‡ç« å†…å®¹ã€‚
    ä¼˜å…ˆä½¿ç”¨ content å­—æ®µï¼Œå…¶æ¬¡ä½¿ç”¨ summary/description å­—æ®µã€‚
    """
    # å°è¯•è·å– contentï¼ˆæŸäº› RSS åŒ…å«å®Œæ•´å†…å®¹ï¼‰
    content = None
    if hasattr(entry, "content") and entry.content:
        # content å¯èƒ½æ˜¯åˆ—è¡¨ï¼Œå–ç¬¬ä¸€ä¸ª
        if isinstance(entry.content, list) and entry.content:
            content = entry.content[0].get("value", "")
        elif isinstance(entry.content, str):
            content = entry.content
    
    # å¦‚æœæ²¡æœ‰ contentï¼Œå°è¯• summary æˆ– description
    if not content:
        content = getattr(entry, "summary", "") or getattr(entry, "description", "") or ""
    
    if not content:
        return None
    
    # æ¸…ç† HTML æ ‡ç­¾ï¼ˆå¦‚æœå†…å®¹æ˜¯ HTMLï¼‰
    # ç®€å•å»é™¤ HTML æ ‡ç­¾
    content = re.sub(r"<[^>]+>", "", content)
    # è§£ç  HTML å®ä½“
    content = html.unescape(content)
    # æ¸…ç†å¤šä½™ç©ºç™½
    content = re.sub(r"\s+", " ", content).strip()
    
    return content if content else None


def crawl_rss_direct(
    article_tags: List[str],
    feed_url: str,
) -> Optional[Dict[str, Any]]:
    """
    é€šç”¨çš„ RSS feed æŠ“å–ç­–ç•¥ï¼ˆé€‚ç”¨äºæ‰€æœ‰æ ‡å‡† RSSï¼‰ï¼š
    - éå† RSS feed ä¸­çš„æ–‡ç« 
    - ç”¨è°ƒç”¨æ–¹ç»™å‡ºçš„ article_tags åšç²—è¿‡æ»¤
    - ç”¨ is_recent åšæ—¶é—´è¿‡æ»¤
    - æŠ“æ­£æ–‡ã€æŠ“å›¾ç‰‡ã€ç”Ÿæˆæ‘˜è¦
    - è¿”å›ç¬¬ä¸€ç¯‡ç¬¦åˆæ¡ä»¶çš„æ–‡ç« æ‘˜è¦ç»“æœ
    """
    feed = _fetch_feed(feed_url)

    for entry in feed.entries:
        title = getattr(entry, "title", "Unknown")
        url = getattr(entry, "link", "")

        print(f"\næ£€æŸ¥æ–‡ç« : {title}")

        title_text = getattr(entry, "title", "") or ""
        summary_text = getattr(entry, "summary", "") or ""
        if not match_tags(title_text, article_tags, summary=summary_text):
            continue

        if not is_recent(entry):
            print("  â­ï¸ è·³è¿‡ï¼ˆä¸åœ¨æ—¶é—´çª—å£å†…ï¼‰")
            continue

        image_url = get_image_url(entry, url)
        
        # ä¼˜å…ˆä» RSS entry ä¸­æå–å†…å®¹ï¼Œå¦‚æœä¸å¤Ÿå†é€šè¿‡ crawl è·å–
        content = _extract_content_from_entry(entry)
        if not content or len(content) < MIN_CONTENT_LENGTH:
            print(f"  ğŸ“„ RSS å†…å®¹ä¸è¶³ï¼ˆ{len(content) if content else 0} å­—ç¬¦ï¼‰ï¼Œé€šè¿‡ crawl è·å–å®Œæ•´æ­£æ–‡...")
            content = extract_article_content(url)
        
        if not content or len(content) < MIN_CONTENT_LENGTH:
            print(
                f"  âš ï¸ å†…å®¹è¿‡çŸ­ï¼ˆ{len(content) if content else 0} å­—ç¬¦ < {MIN_CONTENT_LENGTH}ï¼‰ï¼Œè·³è¿‡"
            )
            continue

        # è¿”å›åŸå§‹å†…å®¹ï¼Œä¸è¿›è¡Œæ€»ç»“ï¼ˆæ€»ç»“ç”±è°ƒç”¨æ–¹è´Ÿè´£ï¼‰
        return {
            "title": title,
            "content": content,
            "image_url": image_url,
            "url": url,
        }

    print("â— æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡ç« ")
    return None

